# ADR-002: Database and Data Storage Strategy

## Status
Accepted

## Context
We needed to decide on a data storage solution for CUNY GradeLens that would handle:
- Grade distribution data from CSV files (thousands of records per semester)
- Professor information (names, courses taught, grade statistics)
- Course information (subject codes, course numbers, names, sections)
- Conversation history (chat sessions, messages, context)
- Search and query performance requirements

Questions to consider:
- Where will grade distribution data be stored?
- How will we handle CSV file ingestion?
- What database structure supports efficient queries?
- How do we handle conversation history persistence?
- What are the trade-offs of different approaches?
- How do we ensure data integrity during rapid development?

Key requirements:
- Fast data ingestion from CSV files
- Efficient queries for professor and course searches
- Support for fuzzy matching and pattern matching
- Conversation history with session management
- No external database server setup (hackathon constraint)
- Easy backup and deployment

## Decision
We will use **SQLite3** as our primary database because:
1. File-based database - no server setup required
2. Perfect for hackathon timeline - immediate availability
3. ACID compliance ensures data integrity
4. Supports complex SQL queries for aggregations
5. Lightweight and fast for our data volume
6. Easy to backup and deploy with application

We will use **two separate SQLite databases**:
1. **grades.db** - Stores all grade distribution data
2. **conversations.db** - Stores chat sessions and conversation history

This separation because:
1. Different access patterns (read-heavy vs write-heavy)
2. Easier to backup conversation data separately
3. Can optimize each database independently
4. Clearer data organization

We will use **CSV Parser library** for data ingestion because:
1. Handles various CSV formats and column name variations
2. Stream-based processing for large files
3. Easy to normalize column names
4. Good error handling for malformed data

We will use **raw SQL queries** instead of an ORM because:
1. Direct control over query performance
2. No additional dependencies
3. Simple schema doesn't require complex relationships
4. Faster development for hackathon timeline

## Consequences

### Positive
- No database server setup - immediate availability
- Fast data ingestion from CSV files
- Efficient queries with proper indexing
- Two-database approach provides clear separation
- Easy to backup and deploy
- SQLite handles our data volume well

### Negative
- SQLite may not scale for very large datasets (millions of records)
- Concurrent write operations limited
- No built-in replication or clustering
- Manual SQL query writing (no type safety)
- Schema changes require manual migration scripts

## Migration Plan

### Day 1: Database Setup
- Created grades.db schema with columns: term, subject, nbr, course_name, section, prof, total, grade columns (A+, A, A-, etc.), avg_gpa
- Created conversations.db schema with sessions and conversations tables
- Implemented CSV ingestion script with column name normalization
- Added error handling for malformed CSV data
- Tested with sample CSV files

### Day 2: Query Optimization
- Added indexes on frequently queried columns (prof, subject, nbr, term)
- Implemented aggregation queries for professor statistics
- Created service layer for database operations
- Added conversation history persistence
- Optimized fuzzy matching queries

### Day 3: Data Quality and Cleanup
- Fixed CSV parsing issues with column name variations
- Improved data validation during ingestion
- Added database statistics endpoints
- Cleaned up duplicate or malformed records
- Verified data integrity across queries

## Database Schema

### grades.db
```sql
CREATE TABLE grades (
  term TEXT,
  subject TEXT,
  nbr TEXT,
  course_name TEXT,
  section TEXT,
  prof TEXT,
  total INTEGER,
  a_plus INTEGER,
  a INTEGER,
  a_minus INTEGER,
  b_plus INTEGER,
  b INTEGER,
  b_minus INTEGER,
  c_plus INTEGER,
  c INTEGER,
  c_minus INTEGER,
  d INTEGER,
  f INTEGER,
  w INTEGER,
  inc_na INTEGER,
  avg_gpa REAL
);
```

### conversations.db
```sql
CREATE TABLE sessions (
  id TEXT PRIMARY KEY,
  user_id TEXT,
  created_at DATETIME,
  last_activity DATETIME,
  metadata TEXT
);

CREATE TABLE conversations (
  id TEXT PRIMARY KEY,
  session_id TEXT,
  user_message TEXT,
  bot_response TEXT,
  professor_name TEXT,
  course_info TEXT,
  grade_data TEXT,
  timestamp DATETIME,
  metadata TEXT
);
```

## Alternatives Considered

### Option 1: PostgreSQL
- **Pros**: Better scalability, more features, ACID compliance, better concurrent writes
- **Cons**: Requires database server setup, more complex deployment, overkill for hackathon
- **Decision**: Chose SQLite for simplicity and immediate availability

### Option 2: MongoDB (NoSQL)
- **Pros**: Flexible schema, easy to start, good for rapid prototyping
- **Cons**: No built-in relationships, eventual consistency concerns, less suitable for structured grade data
- **Decision**: Chose SQLite for relational data integrity and SQL query capabilities

### Option 3: Single Database for All Data
- **Pros**: Simpler architecture, single connection
- **Cons**: Mixed access patterns, harder to optimize, larger database file
- **Decision**: Chose two databases for better organization and optimization

### Option 4: In-Memory Database (Redis)
- **Pros**: Very fast, good for caching
- **Cons**: Data loss on restart, not suitable for persistent storage
- **Decision**: Used SQLite for persistence, Node-cache for in-memory caching

### Option 5: Prisma ORM
- **Pros**: Type-safe queries, migration system, better developer experience
- **Cons**: Additional dependency, learning curve, overkill for SQLite
- **Decision**: Used raw SQLite3 for simplicity and direct control

## Notes
- CSV ingestion required multiple iterations to handle column name variations (TERM vs Term vs term)
- Had to add extensive column mapping logic to normalize CSV headers
- Conversation history was added on Day 2 to enable context-aware responses
- Indexes significantly improved query performance for professor searches
- Considered adding full-text search but fuzzy matching was sufficient
- Future: May migrate to PostgreSQL if data volume grows significantly
- Future: Consider adding database migrations system for schema evolution
- Future: May implement connection pooling if concurrent requests increase

